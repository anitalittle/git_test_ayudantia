# -*- coding: utf-8 -*-
"""Tarea1_Metodos_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xjfIRe0OTMFi65w3vQij6wolJ1ryZTjS

# Tarea 1: Clusters espaciales (RadioTaxis)
Nombres: Ana Little / Mayte Fuentes

**Revisar:**


*   Quedan tiempos de trayectoria negativos
"""

import pandas as pd
import numpy as np
from plotnine import *

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

import matplotlib.pyplot as plt
import folium

df = pd.read_csv('dataTaxi.csv')
print(df.dtypes)
print(df.head())
print(df.tail())
# Datos desde 01-03-14 a 31-07-16

fila_7472 = df.iloc[7472]

# Muestra la fila
print(fila_7472)

"""# Problema 1
Realice una limpieza y selección de variables de los datos. Justifique cada paso realizado. Atención, los datos seleccionados para entrenamiento en este proceso serán definitivos para el resto de los Problemas. A la vez, variables descartadas en este problema si pueden ser
utilizadas en los problemas donde se tiene que explicar los clusters encontrados.
"""

# La variable "Unnamed: 0" no entrega info, por lo que se elimina
df = df.drop(columns=["Unnamed: 0"])

# Determina la suma de los nulos por variable
df.isnull().sum() # No hay valores faltantes

df.nunique()

df[['DoW']].value_counts()
#Cantidad de viajes segun el dia de la semana

# Para trabajar facil con las variables Hora_Inicio y Hora_Fin guardamos su formato
df["Hora_Inicio"] = pd.to_datetime(df["Hora_Inicio"], format='%d-%m-%y %H:%M')
df["Hora_Fin"] = pd.to_datetime(df["Hora_Fin"], format='%d-%m-%y %H:%M')

# Verificar cuantas columnas tienen hora_inicio posterior a hora_fin
df['Hora_Inicio_Antes'] = df['Hora_Inicio'] < df['Hora_Fin']
df['Hora_Inicio_Antes'].value_counts()

print("Porcentaje de columnas con Hora_Inicio > Hora_Fin: ", round(df['Hora_Inicio_Antes'].value_counts()[False]/len(df),4)*100)
# Un 1.72% de los datos tiene un error en estas variables. Como no es un valor alto, los eliminaremos del df
df = df[df['Hora_Inicio_Antes'] != False]
len(df)
df = df.drop(columns='Hora_Inicio_Antes')

"""Tiempo_de_Trayectoria: tiempo que tarda en hacer el viaje."""

# Crear variable Tiempo_de_trayectoria que indica la duracion del viaje

df["Tiempo_de_Trayectoria"] = (df["Hora_Fin"] - df["Hora_Inicio"]).dt.total_seconds() / 60
df.head()

# Extraer la informacion del mes en que se realizo el viaje
df["Mes"] = df["Hora_Inicio"].dt.month
df.head()

df["Año"] = df["Hora_Inicio"].dt.year
df.head()

# Ahora que tenemos Horas, Tiempo_de_trayectoria, Mes y Año, extrajimos toda la info de las variables Hora_Inicio y Hora_Fin por lo que podemos eliminarlas para trabajar con ese formato
df = df.drop(columns=["Hora_Inicio", "Hora_Fin"])
df.head()

# Verificar si hay viajes de tiempos inusuales que puedan ser erroneos

# Definir la duración máxima en minutos para considerar "inusual"
duracion_maxima_minutos = (60*24)  # 1 día en minutos

# Filtrar las filas que tienen duración inusual (más de un día)
viajes_inusuales = df[df["Tiempo_de_Trayectoria"] > duracion_maxima_minutos]

viajes_inusuales # Hay 68 viajes de mas de 24 horas

# Eliminar las filas inusuales del DataFrame original
df = df.drop(viajes_inusuales.index)

df

"""# **Transformar variables categóricas (DoW):**
Para utilizar k-means, debemos tener solo variables numericas -> arreglar DoW
1. Op1: one-hot encoding -> no se recomendaba usar var. binarias
2. Op2: dejar como ordinal (Lunes=1, Martes=2, ...) -> no se si k-means va a interpretar mal las distancias entre dias

*Quizas el mismo problema de la Op2 pasa para Horas, Mes y Año?

ELEGIR SOLO UNA OPCION (PUEDEN HABER MAS)
"""

# # Op1: one-hot encoding
# codificador = OneHotEncoder(drop=None, sparse=False) # ? Podriamos hacerlo con drop="first" por lo de la colinealidad pero es mas dificil interpretar despues

# # Ajustar y transformar los datos
# data_cod = codificador.fit_transform(df[['DoW']])

# # Crear un DataFrame con las columnas codificadas
# df_codificado = pd.DataFrame(data_cod, columns=codificador.categories_[0])

# # Juntar el DataFrame codificado con el DataFrame original
# df_codificado = pd.concat([df, df_codificado], axis=1)

# # Actualizar df
# df = df_codificado
# df.head()

# Op2: dejar como var. ordinal -> no me funcionó el OrdinalEncoder de sklearn :(
# df['DoW'].unique()
ordinal_mapping = {
    'lunes': 1,
    'martes': 2,
    'miÃ©rcoles': 3,
    'jueves': 4,
    'viernes': 5,
    'sÃ¡bado': 6,
    'domingo': 7
}

# Aplicar la conversión a valores ordinales
df['DoW_Ordinal'] = df['DoW'].map(ordinal_mapping)
df = df.drop(columns=["DoW"])
df.head()

"""# Estandarizar

Tienen que estar en escalas similares para que una variable no tome mayor peso solo por su escala (k-means se basa en distancia)


*   Longitudes son cercanas a -70.4 a -71
*   Latitudes son cercanas a -33.2 a -33.9
*   Horas va de 0 a 23
*   distKilometros va de 0.03 a 65.8
*   Tiempo_de_Trayectoria: 1 a 1437
*   Mes: 1-12
*   Año: 2014-2016
*   DoW_Ordinal: 1-7

Se ve que es necesario estandarizar, las escalas no son similares.
- Estandarizar mes, año, dow_ordinal???

Las longitudes y latitudes se encuentran cercanas a estos números, ya que representan la zona geográfica de Santiago de Chile.

ref: https://www.geodatos.net/coordenadas/chile/santiago

Revisión del máximo y mínimo del tiempo de trayectoria del viaje
"""

#df['LatitudPAB'].mean()
#print(df["Tiempo_de_Trayectoria"].min())
df["Tiempo_de_Trayectoria"].min()

fila_min_tiempo = df[df["Tiempo_de_Trayectoria"] == df["Tiempo_de_Trayectoria"].min()]

# Muestra los valores de la fila con el tiempo mínimo
print(fila_min_tiempo)
#muestra que el tiempo de trayectoria minimo es -3907????

# Para ver como estan distribuidas las longitudes y latitudes
# plt.figure(figsize=(10, 6))
# plt.scatter(df['LongitudPAB'], df['LatitudPAB'], color='blue', label='Inicio')
# plt.scatter(df['LongitudCOM'], df['LatitudCOM'], color='red', label='Fin')
# plt.xlabel('Longitud')
# plt.ylabel('Latitud')
# plt.title('Distribución de Coordenadas de Inicio y Fin')
# plt.legend()
# plt.grid(True)
# plt.show()

# Estandarizar
columnas_estandarizar = ['LatitudPAB', 'LongitudPAB', 'LatitudCOM', 'LongitudCOM',
                         'Horas', 'distKilometros', 'Tiempo_de_Trayectoria',
                         'Mes', 'Año', 'DoW_Ordinal'] # estas no se

scaler = StandardScaler()

# Ajustar el scaler a los datos y transformar las columnas seleccionadas
df[columnas_estandarizar] = scaler.fit_transform(df[columnas_estandarizar])
df

"""# Problema 2
Aplique alguno de los algoritmos visto en clases para determinar clusters de trayectoria
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Supongamos que tienes un DataFrame llamado df con las características preparadas

# Seleccionar las características que deseas utilizar en el clustering
features = df[['LatitudPAB', 'LongitudPAB', 'LatitudCOM', 'LongitudCOM', 'Horas', 'distKilometros', 'Tiempo_de_Trayectoria', 'Mes', 'Año', 'DoW_Ordinal']]

# Inicializar una lista para almacenar los valores de la inercia (suma de distancias cuadradas)
inertia_values = []

# Rango de valores de k que deseas probar
k_values = range(1, 20)  # Puedes ajustar este rango según tu caso

# # Entrenar el modelo K-Means para diferentes valores de k y almacenar la inercia
# for k in k_values:
#     kmeans = KMeans(n_clusters=k, random_state=0, n_init=2)
#     kmeans.fit(features)
#     inertia_values.append(kmeans.inertia_)

# # Graficar el método del codo para seleccionar k óptimo
# plt.figure(figsize=(8, 6))
# plt.plot(k_values, inertia_values, marker='o')
# plt.xlabel('Número de Clusters (k)')
# plt.ylabel('Inercia')
# plt.title('Método del Codo para Selección de k')
# plt.xticks(k_values)
# plt.grid(True)
# plt.show()

features2 = df[['LatitudPAB', 'LongitudPAB', 'LatitudCOM', 'LongitudCOM', 'Horas', 'distKilometros', 'Tiempo_de_Trayectoria']]

# Entrenar el modelo K-Means con el número k de clusters
kmeans2 = KMeans(n_clusters=12, random_state=0, n_init=1)
clusters2 = kmeans2.fit_predict(features2)

# Agregar la columna 'Cluster' al DataFrame
df['Cluster'] = clusters2

# Crear un gráfico de dispersión
plt.figure(figsize=(10, 6))

# Colores para los clusters
colors = ['blue', 'green', 'red', 'purple', 'orange', 'darkblue', 'darkgreen', 'darkred', 'cadetblue', 'lightgreen', 'beige', 'pink']

# Agregar puntos para cada viaje en el gráfico de dispersión
for cluster in df['Cluster'].unique():
    cluster_data = df[df['Cluster'] == cluster]
    plt.scatter(cluster_data['LongitudPAB'], cluster_data['LatitudPAB'], color=colors[cluster % len(colors)], label=f'Cluster {cluster}')

plt.title('Clusters de Trayectoria')
plt.xlabel('Longitud')
plt.ylabel('Latitud')
plt.legend()
plt.grid(True)
plt.show()


"""# Problema 3
Explique los clusters encontrados en el problema 2) para una persona que no entiende lo que es un proceso de clusterización.

# Problema 4
Modifique k-means para que una vez que sea aplicado con un valor dado de k, analice
cada cluster y, según alguna regla definida por usted, determine que clusters están mal
definidos/incorrectos. Posteriormente, cada cluster incorrecto sepárelo en dos clusters y vuelva a
correr k-means con el nuevo número de clusters y centroides. Repita este proceso hasta que todos
los clusters encontrados sean considerados correctos.

# Problema 5
Explique los clusters encontrados en el problema 4) para una persona que no entiende
lo que es un proceso de clusterización

# Problema 6
Compare los clusters encontrados en el Problema 2 y Problema 4. Posteriormente,
justifique cual de los métodos seleccionaría para una implementación dentro de la empresa.
"""

#### NO VA AQUI Solo queria probar lo del mapa
subset_df = df.head(1000)

mapa = folium.Map(location=[subset_df['LatitudPAB'].mean(), subset_df['LongitudPAB'].mean()], zoom_start=12)

for index, row in subset_df.iterrows():
    start_coords = [row['LatitudPAB'], row['LongitudPAB']]
    end_coords = [row['LatitudCOM'], row['LongitudCOM']]
    folium.PolyLine(locations=[start_coords, end_coords], color='blue').add_to(mapa)

mapa